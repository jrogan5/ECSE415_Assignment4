{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrogan5/ECSE415_Assignment4/blob/main/AS4_Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# ECSE 415 Assignment 4 - Traffic Sign Detection (Kaggle Version)\n",
        "\n",
        "This notebook has been adapted to run in Kaggle with GPU support.\n",
        "\n",
        "## Setup Instructions for Kaggle:\n",
        "\n",
        "1. **Add Dataset**: Add your \"Road_Signs_Detection_Dataset\" as a dataset to this notebook\n",
        "2. **Update Dataset Path**: In the first code cell, update `BASE_PATH` to match your dataset name\n",
        "3. **Enable GPU**: Settings \u2192 Accelerator \u2192 GPU (T4 or P100)\n",
        "4. **Internet**: Enable if you need to download pretrained models (Settings \u2192 Internet \u2192 On)\n",
        "\n",
        "## What this notebook does:\n",
        "\n",
        "- Trains **YOLOv8** baseline model for traffic sign detection\n",
        "- Trains improved **RetinaNet** models (v1 and v2)\n",
        "- Generates multiple submission files with different confidence thresholds\n",
        "- Evaluates models with mAP, F1 score, and confusion matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Kaggle Environment Setup\n",
        "# ================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect if we're running in Kaggle\n",
        "IS_KAGGLE = os.path.exists('/kaggle')\n",
        "\n",
        "if IS_KAGGLE:\n",
        "    print(\"Running in Kaggle environment\")\n",
        "    # Note: Update 'road-signs-detection' with your actual Kaggle dataset name\n",
        "    # You can find this in the dataset URL or when you add data to your notebook\n",
        "    BASE_PATH = Path(\"/kaggle/input/road-signs-detection\")\n",
        "    OUTPUT_PATH = Path(\"/kaggle/working\")\n",
        "    \n",
        "    # Verify the dataset exists\n",
        "    if not BASE_PATH.exists():\n",
        "        print(f\"WARNING: Dataset not found at {BASE_PATH}\")\n",
        "        print(\"Available input datasets:\")\n",
        "        if Path(\"/kaggle/input\").exists():\n",
        "            for item in Path(\"/kaggle/input\").iterdir():\n",
        "                print(f\"  - {item.name}\")\n",
        "        print(\"\\nPlease update the BASE_PATH variable to match your dataset name.\")\n",
        "else:\n",
        "    print(\"Running in local/Colab environment\")\n",
        "    # For local development - update this path as needed\n",
        "    BASE_PATH = Path(\"./\")\n",
        "    OUTPUT_PATH = Path(\"./\")\n",
        "\n",
        "print(f\"Base path: {BASE_PATH}\")\n",
        "print(f\"Output path: {OUTPUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ================================\n",
        "# GPU Availability Check\n",
        "# ================================\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\u2713 GPU is available!\")\n",
        "    print(f\"  Device name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Device count: {torch.cuda.device_count()}\")\n",
        "    print(f\"  Current device: {torch.cuda.current_device()}\")\n",
        "    print(f\"  Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"  Memory cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"\u26a0 GPU is NOT available. Training will be slow.\")\n",
        "    print(\"  In Kaggle: Go to Settings > Accelerator > Select 'GPU'\")\n",
        "    print(\"  Then restart the notebook session.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmQOeHCFSFqw",
        "outputId": "6a4faa03-358c-415b-dc26-dcc430963a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version:           3.12.12\n",
            "OpenCV version:           4.12.0\n",
            "NumPy version:            2.0.2\n",
            "Matplotlib version:       3.10.0\n",
            "PyTorch version:          2.8.0+cu126\n",
            "scikit-image version:     0.25.2\n",
            "scikit-learn version:     1.6.1\n",
            "Path: C:\\Users\\jrog1\\OneDrive\\Desktop\\Semester 5\\ECSE415\\ECSE415_Assignment4\\\n",
            "Matplotlib backend: module://matplotlib_inline.backend_inline\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "import skimage\n",
        "from skimage import __version__ as skimage_version\n",
        "from sklearn import __version__ as sklearn_version\n",
        "\n",
        "\n",
        "path = str(BASE_PATH)  # Set by environment detection cell above\n",
        "\n",
        "print(f\"Python version:           {os.sys.version.split()[0]}\")\n",
        "print(f\"OpenCV version:           {cv2.__version__}\")\n",
        "print(f\"NumPy version:            {np.__version__}\")\n",
        "print(f\"Matplotlib version:       {matplotlib.__version__}\")\n",
        "print(f\"PyTorch version:          {torch.__version__}\")\n",
        "print(f\"scikit-image version:     {skimage_version}\")\n",
        "print(f\"scikit-learn version:     {sklearn_version}\")\n",
        "print(\"Path: \" + path)\n",
        "\n",
        "# Display sanity helpers\n",
        "from IPython import get_ipython\n",
        "ip = get_ipython()\n",
        "if ip: ip.run_line_magic(\"matplotlib\", \"inline\")\n",
        "\n",
        "import matplotlib, matplotlib.pyplot as plt, numpy as np, cv2\n",
        "\n",
        "# Make inline figures a bit sharper\n",
        "matplotlib.rcParams[\"figure.dpi\"] = 120\n",
        "\n",
        "print(\"Matplotlib backend:\", matplotlib.get_backend())\n",
        "\n",
        "# Preprocessing configuration\n",
        "RESIZE_ENABLED = True\n",
        "TARGET_SIZE = 256\n",
        "\n",
        "# Normalize [0,1] or [-1,1]\n",
        "NORMALIZE_TO_MINUS1_1 = False\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zzC8VNiV4hj",
        "outputId": "5a2e4104-cc86-4983-a148-e9b18bdb5b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images dir : C:\\Users\\jrog1\\OneDrive\\Desktop\\Semester 5\\ECSE415\\ECSE415_Assignment4\\/Road_Signs_Detection_Dataset/train/images\n",
            "Train labels dir : C:\\Users\\jrog1\\OneDrive\\Desktop\\Semester 5\\ECSE415\\ECSE415_Assignment4\\/Road_Signs_Detection_Dataset/train/labels\n",
            "Test images dir  : C:\\Users\\jrog1\\OneDrive\\Desktop\\Semester 5\\ECSE415\\ECSE415_Assignment4\\/Road_Signs_Detection_Dataset/test/images\n",
            "Num train images : 0\n",
            "Num train labels : 0\n",
            "Num test images  : 0\n",
            "Number of classes: 14\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Paths and class names\n",
        "# ================================\n",
        "root = Path(path, \"Road_Signs_Detection_Dataset\")\n",
        "\n",
        "TRAIN_IMAGES = Path(root, \"train\", \"images\")\n",
        "TRAIN_LABELS = Path(root, \"train\", \"labels\")\n",
        "TEST_IMAGES  = Path(root, \"test\", \"images\")\n",
        "\n",
        "print(\"Train images dir :\", TRAIN_IMAGES)\n",
        "print(\"Train labels dir :\", TRAIN_LABELS)\n",
        "print(\"Test images dir  :\", TEST_IMAGES)\n",
        "\n",
        "print(\"Num train images :\", len(list(TRAIN_IMAGES.glob(\"*.jpg\"))))\n",
        "print(\"Num train labels :\", len(list(TRAIN_LABELS.glob(\"*.txt\"))))\n",
        "print(\"Num test images  :\", len(list(TEST_IMAGES.glob(\"*.jpg\"))))\n",
        "\n",
        "class_names = [\n",
        "    \"Speed Limit 80\",   # 0\n",
        "    \"Speed Limit 50\",   # 1\n",
        "    \"Green Light\",      # 2\n",
        "    \"Speed Limit 90\",   # 3\n",
        "    \"Speed Limit 40\",   # 4\n",
        "    \"Speed Limit 120\",  # 5\n",
        "    \"Stop\",             # 6\n",
        "    \"Speed Limit 60\",   # 7\n",
        "    \"Speed Limit 70\",   # 8\n",
        "    \"Speed Limit 20\",   # 9\n",
        "    \"Speed Limit 110\",  # 10\n",
        "    \"Red Light\",        # 11\n",
        "    \"Speed Limit 30\",   # 12\n",
        "    \"Speed Limit 100\"   # 13\n",
        "]\n",
        "num_classes = len(class_names)\n",
        "print(\"Number of classes:\", num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LMvnA6bBZ2oy"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Helper: load YOLO-format labels\n",
        "# ================================\n",
        "def load_yolo_labels(label_path: Path):\n",
        "    \"\"\"\n",
        "    Load YOLO-format labels from a .txt file.\n",
        "    Each line: <class_id> <x_center> <y_center> <width> <height> (normalized)\n",
        "    \"\"\"\n",
        "    boxes = []\n",
        "    if not label_path.exists():\n",
        "        return boxes\n",
        "\n",
        "    with open(label_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls = int(parts[0])\n",
        "            xc, yc, w, h = map(float, parts[1:])\n",
        "            boxes.append((cls, xc, yc, w, h))\n",
        "    return boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTk8cKpEZ6HK",
        "outputId": "8b82a141-afa9-42ba-e98f-376e6ee189b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No images found in C:\\Users\\jrog1\\OneDrive\\Desktop\\Semester 5\\ECSE415\\ECSE415_Assignment4\\/Road_Signs_Detection_Dataset/train/images\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Show sample images with bounding boxes and labels\n",
        "# ================================\n",
        "import random\n",
        "\n",
        "def show_samples_with_boxes(n_samples=8):\n",
        "    image_files = list(TRAIN_IMAGES.glob(\"*.jpg\"))\n",
        "    if len(image_files) == 0:\n",
        "        print(\"No images found in\", TRAIN_IMAGES)\n",
        "        return\n",
        "\n",
        "    random.shuffle(image_files)\n",
        "    image_files = image_files[:n_samples]\n",
        "\n",
        "    rows = 2\n",
        "    cols = (n_samples + 1) // 2\n",
        "\n",
        "    plt.figure(figsize=(4 * cols, 4 * rows))\n",
        "\n",
        "    for i, img_path in enumerate(image_files):\n",
        "        label_path = Path(TRAIN_LABELS, img_path.stem + \".txt\")\n",
        "\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        boxes = load_yolo_labels(label_path)\n",
        "\n",
        "        # draw bounding boxes\n",
        "        for (cls, xc, yc, bw, bh) in boxes:\n",
        "            xc_pix = xc * w\n",
        "            yc_pix = yc * h\n",
        "            bw_pix = bw * w\n",
        "            bh_pix = bh * h\n",
        "\n",
        "            x1 = int(xc_pix - bw_pix / 2)\n",
        "            y1 = int(yc_pix - bh_pix / 2)\n",
        "            x2 = int(xc_pix + bw_pix / 2)\n",
        "            y2 = int(yc_pix + bh_pix / 2)\n",
        "\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            label_text = f\"{cls}: {class_names[cls]}\"\n",
        "            cv2.putText(img, label_text, (x1, max(0, y1 - 5)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(img_path.name)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to visualize samples\n",
        "show_samples_with_boxes(n_samples=8)   #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "BIOMl6UoaiLt",
        "outputId": "fb623caa-07ce-4a4a-f738-55b03e457af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instances per class: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ================================\n",
        "# Count number of instances per class\n",
        "# ================================\n",
        "from collections import Counter\n",
        "\n",
        "def count_instances_per_class():\n",
        "    counts = Counter()\n",
        "    label_files = list(TRAIN_LABELS.glob(\"*.txt\"))\n",
        "    for lf in label_files:\n",
        "        with open(lf, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) == 5:\n",
        "                    cls = int(parts[0])\n",
        "                    counts[cls] += 1\n",
        "    # Convert to list ordered by class index\n",
        "    class_counts = [counts[i] for i in range(num_classes)]\n",
        "    return class_counts\n",
        "\n",
        "class_counts = count_instances_per_class()\n",
        "print(\"Instances per class:\", class_counts)\n",
        "\n",
        "# ================================\n",
        "# Plot bar chart of instances per class\n",
        "# ================================\n",
        "x = np.arange(num_classes)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(x, class_counts)\n",
        "plt.xticks(\n",
        "    x,\n",
        "    [f\"{i}\\n{class_names[i]}\" for i in range(num_classes)],\n",
        "    rotation=45,\n",
        "    ha=\"right\"\n",
        ")\n",
        "plt.ylabel(\"Number of instances\")\n",
        "plt.title(\"Number of instances per class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "3_sjUM-51PlH",
        "outputId": "6533bec9-eb67-461e-b7f2-2ee65654d935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images used for train/val split: 0\n",
            "Example representative classes: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-178686823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m train_imgs, val_imgs, train_labels, val_labels, y_train, y_val = train_test_split(\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mimage_paths_for_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mlabel_paths_for_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# Image Preprocessing Configuration\n",
        "# ================================\n",
        "# Whether to resize images, and the target resolution.\n",
        "RESIZE_ENABLED = True\n",
        "TARGET_SIZE = 256   # e.g., 256; can also be 128 or kept as 416\n",
        "\n",
        "# Pixel normalization range: [0,1] or [-1,1]\n",
        "NORMALIZE_TO_MINUS1_1 = False  # Set True to normalize to [-1,1]\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Image Loading & Preprocessing Function\n",
        "# ================================\n",
        "def load_and_preprocess_image(img_path, resize=RESIZE_ENABLED, target_size=TARGET_SIZE,\n",
        "                              normalize_to_minus1_1=NORMALIZE_TO_MINUS1_1):\n",
        "    \"\"\"\n",
        "    Load an image and apply the following preprocessing steps:\n",
        "    1) Convert from BGR (OpenCV default) to RGB\n",
        "    2) Optionally resize to target_size x target_size\n",
        "    3) Convert to float32\n",
        "    4) Normalize pixel values either to [0,1] or [-1,1]\n",
        "\n",
        "    Returns:\n",
        "        Preprocessed image as a (H, W, 3) float32 numpy array\n",
        "    \"\"\"\n",
        "    img = cv2.imread(str(img_path))\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
        "\n",
        "    # Convert BGR to RGB (required for visualization and consistency)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Optional resizing (dataset is originally 416x416; resizing reduces computation)\n",
        "    if resize:\n",
        "        img = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Convert to float32\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    # Normalize pixel values to [0,1]\n",
        "    img = img / 255.0\n",
        "\n",
        "    # normalize further to [-1,1],But i will use[0,1]\n",
        "    if normalize_to_minus1_1:\n",
        "        img = img * 2.0 - 1.0\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Build Dataset Lists for Train/Val Split\n",
        "# ================================\n",
        "all_image_paths = sorted(TRAIN_IMAGES.glob(\"*.jpg\"))\n",
        "\n",
        "image_paths_for_split = []\n",
        "label_paths_for_split = []\n",
        "rep_classes = []   # Representative class per image for stratified splitting\n",
        "\n",
        "for img_path in all_image_paths:\n",
        "    label_path = Path(TRAIN_LABELS, img_path.stem + \".txt\")\n",
        "    boxes = load_yolo_labels(label_path)\n",
        "\n",
        "    # If an image has no annotations, skip it\n",
        "    if len(boxes) == 0:\n",
        "        continue\n",
        "\n",
        "    # Use the class_id of the first object as the representative class\n",
        "    rep_class = boxes[0][0]\n",
        "\n",
        "    image_paths_for_split.append(img_path)\n",
        "    label_paths_for_split.append(label_path)\n",
        "    rep_classes.append(rep_class)\n",
        "\n",
        "print(\"Number of images used for train/val split:\", len(image_paths_for_split))\n",
        "print(\"Example representative classes:\", rep_classes[:10])\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Train/Validation Split (70/30 with stratification)\n",
        "# ================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_imgs, val_imgs, train_labels, val_labels, y_train, y_val = train_test_split(\n",
        "    image_paths_for_split,\n",
        "    label_paths_for_split,\n",
        "    rep_classes,\n",
        "    test_size=0.3,          # 30% for validation\n",
        "    random_state=42,        # Ensure reproducibility\n",
        "    stratify=rep_classes    # Stratified split to preserve class distribution\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_imgs))\n",
        "print(\"Val size  :\", len(val_imgs))\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Check Class Distribution in Train/Val\n",
        "# ================================\n",
        "from collections import Counter\n",
        "\n",
        "train_class_dist = Counter(y_train)\n",
        "val_class_dist   = Counter(y_val)\n",
        "\n",
        "def pretty_print_class_dist(counter, name):\n",
        "    \"\"\"\n",
        "    Print class distribution with class IDs and names for readability.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{name} class distribution:\")\n",
        "    for cls_id in range(num_classes):\n",
        "        count = counter.get(cls_id, 0)\n",
        "        print(f\"  Class {cls_id:2d} ({class_names[cls_id]:>15}): {count}\")\n",
        "\n",
        "pretty_print_class_dist(train_class_dist, \"Train\")\n",
        "pretty_print_class_dist(val_class_dist, \"Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGU49G3reu7i"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 2.1 Baseline Object Recognition with YOLOv8 (FIXED YAML)\n",
        "# ================================\n",
        "# This cell:\n",
        "#   1) Prepares a YOLO-style train/val split using the lists train_imgs / val_imgs\n",
        "#   2) Creates a dataset YAML file (using a safer list format for 'names')\n",
        "#   3) Fine-tunes a pretrained YOLOv8 model\n",
        "#   4) Evaluates it on the validation set and prints key metrics\n",
        "\n",
        "!pip install -q ultralytics\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# -----------------------------------\n",
        "# Auto-detect GPU availability for YOLO\n",
        "# -----------------------------------\n",
        "import torch\n",
        "cuda_available = torch.cuda.is_available()\n",
        "yolo_device = 0 if cuda_available else 'cpu'\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"YOLO Training Device: {yolo_device}\")\n",
        "if not cuda_available:\n",
        "    print(\"\u26a0\ufe0f  No GPU detected. Training on CPU (slower but works)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# -----------------------------------\n",
        "# 1) Prepare YOLO train/val folder structure using our split\n",
        "# -----------------------------------\n",
        "# We reuse the 'root', 'train_imgs', 'val_imgs', 'train_labels', 'val_labels'\n",
        "# defined in previous cells.\n",
        "\n",
        "yolo_root = Path(OUTPUT_PATH, \"yolo_split\")\n",
        "train_img_dir = Path(yolo_root, \"train\", \"images\")\n",
        "train_lbl_dir = Path(yolo_root, \"train\", \"labels\")\n",
        "val_img_dir   = Path(yolo_root, \"val\", \"images\")\n",
        "val_lbl_dir   = Path(yolo_root, \"val\", \"labels\")\n",
        "\n",
        "# Create directories\n",
        "for d in [train_img_dir, train_lbl_dir, val_img_dir, val_lbl_dir]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def link_or_copy(src, dst):\n",
        "    \"\"\"\n",
        "    Try to create a symlink; if not allowed (e.g., on some systems),\n",
        "    fall back to copying the file.\n",
        "    \"\"\"\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if dst.exists():\n",
        "        return\n",
        "    try:\n",
        "        os.symlink(src, dst)\n",
        "    except OSError:\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "# Populate YOLO train split\n",
        "for img_p, lbl_p in zip(train_imgs, train_labels):\n",
        "    link_or_copy(img_p, Path(train_img_dir, img_p.name))\n",
        "    link_or_copy(lbl_p, Path(train_lbl_dir, lbl_p.name))\n",
        "\n",
        "# Populate YOLO val split\n",
        "for img_p, lbl_p in zip(val_imgs, val_labels):\n",
        "    link_or_copy(img_p, Path(val_img_dir, img_p.name))\n",
        "    link_or_copy(lbl_p, Path(val_lbl_dir, lbl_p.name))\n",
        "\n",
        "print(\"YOLO train images:\", len(list(train_img_dir.glob(\"*.jpg\"))))\n",
        "print(\"YOLO val images  :\", len(list(val_img_dir.glob(\"*.jpg\"))))\n",
        "\n",
        "# -----------------------------------\n",
        "# 2) Create dataset YAML for YOLOv8 (use list format for 'names')\n",
        "# -----------------------------------\n",
        "dataset_yaml_path = Path(yolo_root, \"traffic_signs_yolo.yaml\")\n",
        "\n",
        "dataset_yaml_text = f\"\"\"path: {yolo_root}\n",
        "train: train/images\n",
        "val: val/images\n",
        "\n",
        "names:\n",
        "  - Speed Limit 80\n",
        "  - Speed Limit 50\n",
        "  - Green Light\n",
        "  - Speed Limit 90\n",
        "  - Speed Limit 40\n",
        "  - Speed Limit 120\n",
        "  - Stop\n",
        "  - Speed Limit 60\n",
        "  - Speed Limit 70\n",
        "  - Speed Limit 20\n",
        "  - Speed Limit 110\n",
        "  - Red Light\n",
        "  - Speed Limit 30\n",
        "  - Speed Limit 100\n",
        "\"\"\"\n",
        "\n",
        "with open(dataset_yaml_path, \"w\") as f:\n",
        "    f.write(dataset_yaml_text)\n",
        "\n",
        "print(\"Dataset YAML saved to:\", dataset_yaml_path)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3) Initialize and fine-tune YOLOv8 model\n",
        "# -----------------------------------\n",
        "model = YOLO(\"yolov8n.pt\")  # nano model, pretrained on COCO\n",
        "\n",
        "train_results = model.train(\n",
        "    data=str(dataset_yaml_path),\n",
        "    imgsz=416,\n",
        "    epochs=30,                      # you can reduce to ~20 if time is tight\n",
        "    batch=16,\n",
        "    lr0=1e-3,\n",
        "    device=yolo_device,             # Auto-detected: 0 for GPU, 'cpu' for CPU\n",
        "        project=str(OUTPUT_PATH / \"runs_yolov8_baseline\"),\n",
        "    name=\"yolov8n_traffic_signs\",\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# -----------------------------------\n",
        "# 4) Evaluate the baseline model on the validation set\n",
        "# -----------------------------------\n",
        "val_metrics = model.val(\n",
        "    data=str(dataset_yaml_path),\n",
        "    imgsz=416,\n",
        "    batch=16,\n",
        "    device=yolo_device,             # Auto-detected: 0 for GPU, 'cpu' for CPU\n",
        "        project=str(OUTPUT_PATH / \"runs_yolov8_baseline\"),\n",
        "    name=\"yolov8n_traffic_signs_val\",\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "# Access main metrics from Ultralytics\n",
        "mean_precision = val_metrics.box.mp\n",
        "mean_recall    = val_metrics.box.mr\n",
        "f1_score       = 2 * mean_precision * mean_recall / (mean_precision + mean_recall + 1e-16)\n",
        "\n",
        "print(\"\\n=== Baseline YOLOv8 Validation Metrics ===\")\n",
        "print(f\"mAP@50-95 (all classes): {val_metrics.box.map:.4f}\")\n",
        "print(f\"mAP@50    (all classes): {val_metrics.box.map50:.4f}\")\n",
        "print(f\"Mean Precision        : {mean_precision:.4f}\")\n",
        "print(f\"Mean Recall           : {mean_recall:.4f}\")\n",
        "print(f\"F1 score (overall)    : {f1_score:.4f}\")\n",
        "\n",
        "print(\"\\nPer-class AP (Ultralytics 'maps', typically AP@50-95 per class):\")\n",
        "for i, ap in enumerate(val_metrics.box.maps):\n",
        "    print(f\"  Class {i:2d} ({class_names[i]:>15}): {ap:.4f}\")\n",
        "\n",
        "cm_dir = Path(root, \"runs_yolov8_baseline\", \"yolov8n_traffic_signs_val\")\n",
        "print(\"\\nConfusion matrix and other plots are saved in:\")\n",
        "print(cm_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEILZwGTkf6T"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# 2.2 Analysis of Baseline Results\n",
        "# ================================\n",
        "# This cell:\n",
        "#   1) Visualizes YOLOv8 predictions on a subset of validation images\n",
        "#   2) Displays the confusion matrix image saved by Ultralytics\n",
        "#   3) (Optionally) prints again the main metrics if available\n",
        "\n",
        "# -----------------------------------\n",
        "# 1) Visualize predictions on validation images\n",
        "# -----------------------------------\n",
        "# We use the existing 'val_imgs' list and the trained 'model' from 2.1.\n",
        "\n",
        "num_samples = 8  # number of validation images to visualize (5\u201310 is fine)\n",
        "sample_val_imgs = val_imgs[:num_samples]\n",
        "\n",
        "rows = 2\n",
        "cols = (num_samples + 1) // 2\n",
        "\n",
        "plt.figure(figsize=(4 * cols, 4 * rows))\n",
        "\n",
        "for i, img_path in enumerate(sample_val_imgs):\n",
        "    # Run inference\n",
        "    results = model(str(img_path))\n",
        "    # Plot predictions on the image (Ultralytics returns BGR image)\n",
        "    plotted_bgr = results[0].plot()\n",
        "    plotted_rgb = cv2.cvtColor(plotted_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.imshow(plotted_rgb)\n",
        "    plt.title(f\"Val prediction: {img_path.name}\", fontsize=9)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------\n",
        "# 2) Show confusion matrix generated by Ultralytics\n",
        "# -----------------------------------\n",
        "cm_dir = Path(root, \"runs_yolov8_baseline\", \"yolov8n_traffic_signs_val\")\n",
        "cm_files = list(cm_dir.glob(\"confusion_matrix*.png\"))\n",
        "\n",
        "if len(cm_files) > 0:\n",
        "    cm_img = cv2.imread(str(cm_files[0]))\n",
        "    cm_img = cv2.cvtColor(cm_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(cm_img)\n",
        "    plt.title(\"Confusion Matrix on Validation Set\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No confusion matrix image found in:\", cm_dir)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3) (Optional) Print main metrics again if 'val_metrics' exists\n",
        "# -----------------------------------\n",
        "try:\n",
        "    mean_precision = val_metrics.box.mp\n",
        "    mean_recall    = val_metrics.box.mr\n",
        "    f1_score       = 2 * mean_precision * mean_recall / (mean_precision + mean_recall + 1e-16)\n",
        "\n",
        "    print(\"\\n=== Baseline YOLOv8 Validation Metrics (Summary) ===\")\n",
        "    print(f\"mAP@50-95 (all classes): {val_metrics.box.map:.4f}\")\n",
        "    print(f\"mAP@50    (all classes): {val_metrics.box.map50:.4f}\")\n",
        "    print(f\"Mean Precision        : {mean_precision:.4f}\")\n",
        "    print(f\"Mean Recall           : {mean_recall:.4f}\")\n",
        "    print(f\"F1 score (overall)    : {f1_score:.4f}\")\n",
        "except NameError:\n",
        "    print(\"\\n'val_metrics' not found. Please run the 2.1 cell (training + validation) first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vey3rdPbmwi7"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Part 3 - Optimized RetinaNet Model (with valid box check)\n",
        "# ================================\n",
        "# 3.1 Model: RetinaNet (ResNet-50 FPN) with COCO-pretrained weights\n",
        "# 3.2 Data Augmentation: flip + scale + brightness jitter\n",
        "# 3.3 Training: longer training with scheduler, track train/val loss\n",
        "# ================================\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "RETINA_IMG_SIZE = 416  # fixed training size\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "# Custom Dataset for RetinaNet\n",
        "# -----------------------------------\n",
        "class TrafficSignsRetinaNetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for traffic sign detection using RetinaNet.\n",
        "    It reads images and YOLO-format labels and converts them to\n",
        "    the format expected by torchvision detection models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, label_paths, is_train=True,\n",
        "                 img_size=RETINA_IMG_SIZE, num_classes=num_classes):\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.is_train = is_train\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def _load_image_and_boxes(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        lbl_path = self.label_paths[idx]\n",
        "\n",
        "        # Load image (BGR -> RGB)\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Resize to fixed base size\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Load YOLO-format labels\n",
        "        yolo_boxes = load_yolo_labels(lbl_path)\n",
        "\n",
        "        boxes_xyxy = []\n",
        "        labels = []\n",
        "\n",
        "        for (cls_id, xc, yc, bw, bh) in yolo_boxes:\n",
        "            # Convert normalized center-width-height to absolute xyxy\n",
        "            x_c = xc * self.img_size\n",
        "            y_c = yc * self.img_size\n",
        "            w   = bw * self.img_size\n",
        "            h   = bh * self.img_size\n",
        "\n",
        "            x1 = x_c - w / 2.0\n",
        "            y1 = y_c - h / 2.0\n",
        "            x2 = x_c + w / 2.0\n",
        "            y2 = y_c + h / 2.0\n",
        "\n",
        "            # Clip to image boundaries\n",
        "            x1 = max(0, min(self.img_size - 1, x1))\n",
        "            y1 = max(0, min(self.img_size - 1, y1))\n",
        "            x2 = max(0, min(self.img_size - 1, x2))\n",
        "            y2 = max(0, min(self.img_size - 1, y2))\n",
        "\n",
        "            boxes_xyxy.append([x1, y1, x2, y2])\n",
        "            # RetinaNet expects labels in [1, num_classes]; 0 is background\n",
        "            labels.append(cls_id + 1)\n",
        "\n",
        "        boxes_xyxy = np.array(boxes_xyxy, dtype=np.float32)\n",
        "        labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "        return img, boxes_xyxy, labels\n",
        "\n",
        "    def _apply_augmentations(self, img, boxes):\n",
        "        \"\"\"\n",
        "        Data augmentation for training:\n",
        "          - random horizontal flip\n",
        "          - random scale (0.8~1.2) with padding/cropping\n",
        "          - random brightness scaling\n",
        "        \"\"\"\n",
        "        h, w, _ = img.shape  # (img_size, img_size, 3)\n",
        "\n",
        "        # ----- Random horizontal flip -----\n",
        "        if random.random() < 0.5:\n",
        "            img = np.ascontiguousarray(img[:, ::-1, :])\n",
        "            x1 = boxes[:, 0].copy()\n",
        "            x2 = boxes[:, 2].copy()\n",
        "            boxes[:, 0] = w - x2\n",
        "            boxes[:, 2] = w - x1\n",
        "\n",
        "        # ----- Random scale -----\n",
        "        if random.random() < 0.5:\n",
        "            scale = 0.8 + 0.4 * random.random()  # [0.8, 1.2]\n",
        "            new_size = int(self.img_size * scale)\n",
        "\n",
        "            # Resize image\n",
        "            scaled_img = cv2.resize(img, (new_size, new_size), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Scale boxes coordinates\n",
        "            boxes *= scale\n",
        "\n",
        "            # Put scaled image into a fixed-size canvas\n",
        "            canvas = np.zeros((self.img_size, self.img_size, 3), dtype=scaled_img.dtype)\n",
        "            h_s = min(self.img_size, new_size)\n",
        "            w_s = min(self.img_size, new_size)\n",
        "            canvas[:h_s, :w_s] = scaled_img[:h_s, :w_s]\n",
        "            img = canvas\n",
        "\n",
        "            # Clip boxes to canvas\n",
        "            boxes[:, 0::2] = np.clip(boxes[:, 0::2], 0, self.img_size - 1)\n",
        "            boxes[:, 1::2] = np.clip(boxes[:, 1::2], 0, self.img_size - 1)\n",
        "\n",
        "        # ----- Brightness jitter -----\n",
        "        if random.random() < 0.3:\n",
        "            factor = 0.8 + 0.4 * random.random()  # [0.8, 1.2]\n",
        "            img = img.astype(np.float32) * factor\n",
        "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "        return img, boxes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load base image and boxes\n",
        "        img, boxes_xyxy, labels = self._load_image_and_boxes(idx)\n",
        "\n",
        "        # Keep a copy of original boxes/labels in case augmentations kill all boxes\n",
        "        orig_boxes = boxes_xyxy.copy()\n",
        "        orig_labels = labels.copy()\n",
        "\n",
        "        # Apply augmentations only during training\n",
        "        if self.is_train:\n",
        "            img, boxes_xyxy = self._apply_augmentations(img, boxes_xyxy)\n",
        "\n",
        "        # ---- Ensure all boxes are valid (positive width and height) ----\n",
        "        x1 = boxes_xyxy[:, 0]\n",
        "        y1 = boxes_xyxy[:, 1]\n",
        "        x2 = boxes_xyxy[:, 2]\n",
        "        y2 = boxes_xyxy[:, 3]\n",
        "        valid = (x2 > x1) & (y2 > y1)\n",
        "\n",
        "        if not np.any(valid):\n",
        "            # If all boxes became invalid after augmentation, fall back to original\n",
        "            boxes_xyxy = orig_boxes\n",
        "            labels = orig_labels\n",
        "        else:\n",
        "            boxes_xyxy = boxes_xyxy[valid]\n",
        "            labels = labels[valid]\n",
        "\n",
        "        # Convert image to float tensor [0,1], shape (3, H, W)\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img_tensor = torch.from_numpy(img).permute(2, 0, 1)  # (C,H,W)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": torch.as_tensor(boxes_xyxy, dtype=torch.float32),\n",
        "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
        "        }\n",
        "\n",
        "        return img_tensor, target\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "# Collate function for detection models\n",
        "# -----------------------------------\n",
        "def detection_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for detection models.\n",
        "    Returns:\n",
        "        images: list of tensors, each of shape (3, H, W)\n",
        "        targets: list of dicts\n",
        "    \"\"\"\n",
        "    images, targets = list(zip(*batch))\n",
        "    return list(images), list(targets)\n",
        "\n",
        "\n",
        "# -----------------------------------\n",
        "# Build train/val datasets and dataloaders\n",
        "# -----------------------------------\n",
        "train_dataset = TrafficSignsRetinaNetDataset(train_imgs, train_labels, is_train=True)\n",
        "val_dataset   = TrafficSignsRetinaNetDataset(val_imgs,   val_labels,   is_train=False)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,       # A100 \u4e00\u822c\u53ef\u4ee5 8\uff0c\u4e0d\u591f\u5c31\u6539\u6210 4\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=detection_collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=detection_collate_fn\n",
        ")\n",
        "\n",
        "print(\"RetinaNet train images:\", len(train_dataset))\n",
        "print(\"RetinaNet val images  :\", len(val_dataset))\n",
        "\n",
        "# -----------------------------------\n",
        "# 3.1 Initialize RetinaNet model with COCO-pretrained weights\n",
        "# -----------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "weights = RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
        "model_retina = retinanet_resnet50_fpn(weights=weights)  # COCO, 91 classes\n",
        "\n",
        "# Replace only the final classification conv to match our num_classes+1\n",
        "cls_head = model_retina.head.classification_head\n",
        "in_channels = cls_head.cls_logits.in_channels\n",
        "num_anchors = cls_head.num_anchors\n",
        "\n",
        "new_num_classes = num_classes + 1  # 14 traffic sign classes + background\n",
        "\n",
        "cls_head.cls_logits = nn.Conv2d(\n",
        "    in_channels,\n",
        "    num_anchors * new_num_classes,\n",
        "    kernel_size=3,\n",
        "    stride=1,\n",
        "    padding=1\n",
        ")\n",
        "cls_head.num_classes = new_num_classes\n",
        "\n",
        "model_retina.to(device)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3.3 Training configuration\n",
        "# -----------------------------------\n",
        "optimizer = torch.optim.AdamW(model_retina.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "num_epochs = 50\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer, step_size=30, gamma=0.1\n",
        ")\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# -----------------------------------\n",
        "# 3.3 Train the RetinaNet model\n",
        "# -----------------------------------\n",
        "for epoch in range(num_epochs):\n",
        "    # ===== Train phase =====\n",
        "    model_retina.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Forward pass: RetinaNet returns a dict of losses in training mode\n",
        "        loss_dict = model_retina(images, targets)\n",
        "        losses = sum(loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += losses.item()\n",
        "\n",
        "    avg_train_loss = running_train_loss / max(1, len(train_loader))\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # ===== Validation phase (compute loss only, no gradient) =====\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        # Keep model in train() so that it returns loss dict\n",
        "        model_retina.train()\n",
        "        for images, targets in val_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model_retina(images, targets)\n",
        "            losses = sum(loss_dict.values())\n",
        "            running_val_loss += losses.item()\n",
        "\n",
        "    avg_val_loss = running_val_loss / max(1, len(val_loader))\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    # Step LR scheduler\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]  \"\n",
        "          f\"Train loss: {avg_train_loss:.4f}  \"\n",
        "          f\"Val loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "print(\"Optimized RetinaNet training finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Part 3B - IMPROVED RetinaNet Training with Better Hyperparameters\n",
        "# ================================\n",
        "# Improvements over the previous training:\n",
        "# 1. More aggressive data augmentation\n",
        "# 2. Longer training with better learning rate schedule\n",
        "# 3. Gradient clipping to stabilize training\n",
        "# 4. Class weights to handle imbalance\n",
        "# ================================\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn, RetinaNet_ResNet50_FPN_Weights\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# Improved dataset with more augmentations\n",
        "class ImprovedTrafficSignsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Enhanced dataset with more aggressive data augmentation for better generalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, label_paths, is_train=True,\n",
        "                 img_size=416, num_classes=14):\n",
        "        self.image_paths = image_paths\n",
        "        self.label_paths = label_paths\n",
        "        self.is_train = is_train\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def _load_image_and_boxes(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        lbl_path = self.label_paths[idx]\n",
        "\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        yolo_boxes = load_yolo_labels(lbl_path)\n",
        "\n",
        "        boxes_xyxy = []\n",
        "        labels = []\n",
        "\n",
        "        for (cls_id, xc, yc, bw, bh) in yolo_boxes:\n",
        "            x_c = xc * self.img_size\n",
        "            y_c = yc * self.img_size\n",
        "            w   = bw * self.img_size\n",
        "            h   = bh * self.img_size\n",
        "\n",
        "            x1 = x_c - w / 2.0\n",
        "            y1 = y_c - h / 2.0\n",
        "            x2 = x_c + w / 2.0\n",
        "            y2 = y_c + h / 2.0\n",
        "\n",
        "            x1 = max(0, min(self.img_size - 1, x1))\n",
        "            y1 = max(0, min(self.img_size - 1, y1))\n",
        "            x2 = max(0, min(self.img_size - 1, x2))\n",
        "            y2 = max(0, min(self.img_size - 1, y2))\n",
        "\n",
        "            boxes_xyxy.append([x1, y1, x2, y2])\n",
        "            labels.append(cls_id + 1)  # [1, num_classes]\n",
        "\n",
        "        boxes_xyxy = np.array(boxes_xyxy, dtype=np.float32)\n",
        "        labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "        return img, boxes_xyxy, labels\n",
        "\n",
        "    def _apply_augmentations(self, img, boxes):\n",
        "        \"\"\"\n",
        "        More aggressive data augmentation:\n",
        "          - Horizontal flip (50%)\n",
        "          - Random scale (0.75~1.25)\n",
        "          - Random brightness (0.7~1.3)\n",
        "          - Random contrast (0.7~1.3)\n",
        "          - Color jitter\n",
        "        \"\"\"\n",
        "        h, w, _ = img.shape\n",
        "\n",
        "        # Horizontal flip\n",
        "        if random.random() < 0.5:\n",
        "            img = np.ascontiguousarray(img[:, ::-1, :])\n",
        "            x1 = boxes[:, 0].copy()\n",
        "            x2 = boxes[:, 2].copy()\n",
        "            boxes[:, 0] = w - x2\n",
        "            boxes[:, 2] = w - x1\n",
        "\n",
        "        # Random scale with larger range\n",
        "        if random.random() < 0.6:\n",
        "            scale = 0.75 + 0.5 * random.random()  # [0.75, 1.25]\n",
        "            new_size = int(self.img_size * scale)\n",
        "\n",
        "            scaled_img = cv2.resize(img, (new_size, new_size), interpolation=cv2.INTER_LINEAR)\n",
        "            boxes *= scale\n",
        "\n",
        "            canvas = np.zeros((self.img_size, self.img_size, 3), dtype=scaled_img.dtype)\n",
        "            h_s = min(self.img_size, new_size)\n",
        "            w_s = min(self.img_size, new_size)\n",
        "            canvas[:h_s, :w_s] = scaled_img[:h_s, :w_s]\n",
        "            img = canvas\n",
        "\n",
        "            boxes[:, 0::2] = np.clip(boxes[:, 0::2], 0, self.img_size - 1)\n",
        "            boxes[:, 1::2] = np.clip(boxes[:, 1::2], 0, self.img_size - 1)\n",
        "\n",
        "        # Brightness jitter\n",
        "        if random.random() < 0.5:\n",
        "            factor = 0.7 + 0.6 * random.random()  # [0.7, 1.3]\n",
        "            img = img.astype(np.float32) * factor\n",
        "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Contrast adjustment\n",
        "        if random.random() < 0.3:\n",
        "            alpha = 0.7 + 0.6 * random.random()  # [0.7, 1.3]\n",
        "            img = img.astype(np.float32)\n",
        "            img = 128 + alpha * (img - 128)\n",
        "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Color jitter (saturation)\n",
        "        if random.random() < 0.3:\n",
        "            hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
        "            hsv[:, :, 1] *= (0.8 + 0.4 * random.random())  # [0.8, 1.2]\n",
        "            hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 255)\n",
        "            img = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
        "\n",
        "        return img, boxes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, boxes_xyxy, labels = self._load_image_and_boxes(idx)\n",
        "\n",
        "        orig_boxes = boxes_xyxy.copy()\n",
        "        orig_labels = labels.copy()\n",
        "\n",
        "        if self.is_train:\n",
        "            img, boxes_xyxy = self._apply_augmentations(img, boxes_xyxy)\n",
        "\n",
        "        # Validate boxes\n",
        "        x1 = boxes_xyxy[:, 0]\n",
        "        y1 = boxes_xyxy[:, 1]\n",
        "        x2 = boxes_xyxy[:, 2]\n",
        "        y2 = boxes_xyxy[:, 3]\n",
        "        valid = (x2 > x1) & (y2 > y1)\n",
        "\n",
        "        if not np.any(valid):\n",
        "            boxes_xyxy = orig_boxes\n",
        "            labels = orig_labels\n",
        "        else:\n",
        "            boxes_xyxy = boxes_xyxy[valid]\n",
        "            labels = labels[valid]\n",
        "\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": torch.as_tensor(boxes_xyxy, dtype=torch.float32),\n",
        "            \"labels\": torch.as_tensor(labels, dtype=torch.int64),\n",
        "        }\n",
        "\n",
        "        return img_tensor, target\n",
        "\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAINING IMPROVED RETINANET MODEL\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create improved datasets\n",
        "train_dataset_v2 = ImprovedTrafficSignsDataset(train_imgs, train_labels, is_train=True)\n",
        "val_dataset_v2   = ImprovedTrafficSignsDataset(val_imgs, val_labels, is_train=False)\n",
        "\n",
        "train_loader_v2 = DataLoader(\n",
        "    train_dataset_v2,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=detection_collate_fn\n",
        ")\n",
        "\n",
        "val_loader_v2 = DataLoader(\n",
        "    val_dataset_v2,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=detection_collate_fn\n",
        ")\n",
        "\n",
        "# Initialize improved model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "weights = RetinaNet_ResNet50_FPN_Weights.DEFAULT\n",
        "model_retina_v2 = retinanet_resnet50_fpn(weights=weights)\n",
        "\n",
        "# Replace classification head\n",
        "cls_head = model_retina_v2.head.classification_head\n",
        "in_channels = cls_head.cls_logits.in_channels\n",
        "num_anchors = cls_head.num_anchors\n",
        "new_num_classes = num_classes + 1\n",
        "\n",
        "cls_head.cls_logits = nn.Conv2d(\n",
        "    in_channels,\n",
        "    num_anchors * new_num_classes,\n",
        "    kernel_size=3,\n",
        "    stride=1,\n",
        "    padding=1\n",
        ")\n",
        "cls_head.num_classes = new_num_classes\n",
        "\n",
        "model_retina_v2.to(device)\n",
        "\n",
        "# Improved optimizer with higher initial LR\n",
        "optimizer_v2 = torch.optim.AdamW(\n",
        "    model_retina_v2.parameters(),\n",
        "    lr=2e-4,  # Slightly higher than before\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "# Better learning rate schedule\n",
        "num_epochs_v2 = 60\n",
        "lr_scheduler_v2 = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer_v2,\n",
        "    milestones=[30, 45],  # Decay at epochs 30 and 45\n",
        "    gamma=0.1\n",
        ")\n",
        "\n",
        "train_losses_v2 = []\n",
        "val_losses_v2 = []\n",
        "\n",
        "print(f\"\\nTraining for {num_epochs_v2} epochs with improved augmentation...\")\n",
        "print(f\"Initial LR: {optimizer_v2.param_groups[0]['lr']}\")\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "for epoch in range(num_epochs_v2):\n",
        "    # Train\n",
        "    model_retina_v2.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    for images, targets in train_loader_v2:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model_retina_v2(images, targets)\n",
        "        losses = sum(loss_dict.values())\n",
        "\n",
        "        optimizer_v2.zero_grad()\n",
        "        losses.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model_retina_v2.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer_v2.step()\n",
        "\n",
        "        running_train_loss += losses.item()\n",
        "\n",
        "    avg_train_loss = running_train_loss / max(1, len(train_loader_v2))\n",
        "    train_losses_v2.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        model_retina_v2.train()  # Keep in train mode to get loss\n",
        "        for images, targets in val_loader_v2:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            loss_dict = model_retina_v2(images, targets)\n",
        "            losses = sum(loss_dict.values())\n",
        "            running_val_loss += losses.item()\n",
        "\n",
        "    avg_val_loss = running_val_loss / max(1, len(val_loader_v2))\n",
        "    val_losses_v2.append(avg_val_loss)\n",
        "\n",
        "    lr_scheduler_v2.step()\n",
        "\n",
        "    current_lr = optimizer_v2.param_groups[0]['lr']\n",
        "\n",
        "    if (epoch + 1) % 5 == 0 or epoch < 5:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs_v2}]  \"\n",
        "              f\"Train loss: {avg_train_loss:.4f}  \"\n",
        "              f\"Val loss: {avg_val_loss:.4f}  \"\n",
        "              f\"LR: {current_lr:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"IMPROVED RETINANET TRAINING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses_v2, label='Train Loss (v2)')\n",
        "plt.plot(val_losses_v2, label='Val Loss (v2)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Improved RetinaNet Training Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "epochs_range = range(1, len(train_losses_v2) + 1)\n",
        "plt.plot(epochs_range, train_losses_v2, 'b-', label='Train Loss (v2)', alpha=0.7)\n",
        "plt.plot(epochs_range, val_losses_v2, 'r-', label='Val Loss (v2)', alpha=0.7)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (log scale)')\n",
        "plt.yscale('log')\n",
        "plt.title('Training Curves (Log Scale)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal train loss: {train_losses_v2[-1]:.4f}\")\n",
        "print(f\"Final val loss: {val_losses_v2[-1]:.4f}\")\n",
        "print(f\"Best val loss: {min(val_losses_v2):.4f} at epoch {np.argmin(val_losses_v2) + 1}\")"
      ],
      "metadata": {
        "id": "v55DxV07VUUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn5aVGl5ytgc"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Part 4 - Model Evaluation for RetinaNet (fixed background handling)\n",
        "# ================================\n",
        "# 4.1 Evaluate Performance (F1, mAP@50, mAP@50\u201395, confusion matrix)\n",
        "# 4.2 Visualize Results (validation images with predictions)\n",
        "# 4.3 Generalization Test (external traffic sign image)\n",
        "# ================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Utility: IoU computation ----------\n",
        "def box_iou(box, boxes):\n",
        "    \"\"\"\n",
        "    Compute IoU between one box and an array of boxes.\n",
        "    box:  (4,)  [x1, y1, x2, y2]\n",
        "    boxes: (N,4)\n",
        "    \"\"\"\n",
        "    x1 = np.maximum(box[0], boxes[:, 0])\n",
        "    y1 = np.maximum(box[1], boxes[:, 1])\n",
        "    x2 = np.minimum(box[2], boxes[:, 2])\n",
        "    y2 = np.minimum(box[3], boxes[:, 3])\n",
        "\n",
        "    inter_w = np.maximum(0.0, x2 - x1)\n",
        "    inter_h = np.maximum(0.0, y2 - y1)\n",
        "    inter = inter_w * inter_h\n",
        "\n",
        "    area_box = (box[2] - box[0]) * (box[3] - box[1])\n",
        "    area_boxes = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "\n",
        "    union = area_box + area_boxes - inter + 1e-16\n",
        "    iou = inter / union\n",
        "    return iou\n",
        "\n",
        "\n",
        "# ---------- Utility: AP computation (VOC-style) ----------\n",
        "def voc_ap(rec, prec):\n",
        "    \"\"\"\n",
        "    Compute AP given precision and recall using the VOC 2010+ method.\n",
        "    rec, prec: 1D numpy arrays\n",
        "    \"\"\"\n",
        "    mrec = np.concatenate(([0.0], rec, [1.0]))\n",
        "    mpre = np.concatenate(([0.0], prec, [0.0]))\n",
        "\n",
        "    # Make precision monotonically decreasing\n",
        "    for i in range(mpre.size - 1, 0, -1):\n",
        "        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n",
        "\n",
        "    idx = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n",
        "    return ap\n",
        "\n",
        "\n",
        "# ---------- 4.1 Evaluate RetinaNet on validation set ----------\n",
        "def evaluate_retinanet(model, dataset, device,\n",
        "                       iou_thresholds=np.arange(0.5, 1.0, 0.05),\n",
        "                       score_thresh=0.05):\n",
        "    \"\"\"\n",
        "    Evaluate RetinaNet on the validation set:\n",
        "      - per-class Precision, Recall, F1\n",
        "      - per-class AP@50 and AP@50\u201395\n",
        "      - overall mAP@50 and mAP@50\u201395 (macro over classes)\n",
        "    Note: label 0 is treated as background and ignored.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        collate_fn=detection_collate_fn\n",
        "    )\n",
        "\n",
        "    # labels in dataset are [1..num_classes]; 0 is background (not present in GT)\n",
        "    num_cls_with_bg = num_classes + 1\n",
        "    gt_boxes_per_image = {}\n",
        "    gt_labels_per_image = {}\n",
        "    gt_count_per_class = {c: 0 for c in range(1, num_cls_with_bg)}\n",
        "\n",
        "    # ----- Ground truth stats -----\n",
        "    for img_idx in range(len(dataset)):\n",
        "        _, target = dataset[img_idx]\n",
        "        boxes = target[\"boxes\"].cpu().numpy()\n",
        "        labels = target[\"labels\"].cpu().numpy()  # [1..num_classes]\n",
        "\n",
        "        gt_boxes_per_image[img_idx] = boxes\n",
        "        gt_labels_per_image[img_idx] = labels\n",
        "        for l in labels:\n",
        "            gt_count_per_class[int(l)] += 1\n",
        "\n",
        "    # ----- Collect predictions per foreground class -----\n",
        "    preds_per_class = {c: [] for c in range(1, num_cls_with_bg)}  # only 1..num_classes\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_idx, (images, _) in enumerate(eval_loader):\n",
        "            img = images[0].to(device)\n",
        "            outputs = model([img])[0]\n",
        "\n",
        "            boxes = outputs[\"boxes\"].cpu().numpy()\n",
        "            scores = outputs[\"scores\"].cpu().numpy()\n",
        "            labels = outputs[\"labels\"].cpu().numpy()  # may include 0 (background)\n",
        "\n",
        "            # filter by score and remove background (label==0)\n",
        "            keep = (scores >= score_thresh) & (labels != 0)\n",
        "            boxes = boxes[keep]\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "\n",
        "            for b, s, l in zip(boxes, scores, labels):\n",
        "                c = int(l)\n",
        "                if c < 1 or c >= num_cls_with_bg:\n",
        "                    continue\n",
        "                preds_per_class[c].append({\n",
        "                    \"image_id\": img_idx,\n",
        "                    \"score\": float(s),\n",
        "                    \"box\": b\n",
        "                })\n",
        "\n",
        "    # ----- Compute per-class AP for each IoU threshold -----\n",
        "    ap50_per_class = {c: 0.0 for c in range(1, num_cls_with_bg)}\n",
        "    ap5095_per_class = {c: 0.0 for c in range(1, num_cls_with_bg)}\n",
        "\n",
        "    TP_50 = {c: 0 for c in range(1, num_cls_with_bg)}\n",
        "    FP_50 = {c: 0 for c in range(1, num_cls_with_bg)}\n",
        "    FN_50 = {c: 0 for c in range(1, num_cls_with_bg)}\n",
        "\n",
        "    for c in range(1, num_cls_with_bg):\n",
        "        if gt_count_per_class[c] == 0:\n",
        "            continue\n",
        "\n",
        "        preds_c = preds_per_class[c]\n",
        "        if len(preds_c) == 0:\n",
        "            continue\n",
        "\n",
        "        preds_c = sorted(preds_c, key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "        # Precompute GT boxes of this class for every image\n",
        "        gt_boxes_c = {}\n",
        "        for img_id in range(len(dataset)):\n",
        "            labels = gt_labels_per_image[img_id]\n",
        "            boxes = gt_boxes_per_image[img_id]\n",
        "            mask = labels == c\n",
        "            gt_boxes_c[img_id] = boxes[mask]\n",
        "\n",
        "        ap_per_iou = []\n",
        "\n",
        "        for thr_idx, iou_thr in enumerate(iou_thresholds):\n",
        "            n_pred = len(preds_c)\n",
        "            TP = np.zeros(n_pred, dtype=np.float32)\n",
        "            FP = np.zeros(n_pred, dtype=np.float32)\n",
        "\n",
        "            used = {img_id: np.zeros(len(gt_boxes_c[img_id]), dtype=bool)\n",
        "                    for img_id in range(len(dataset))}\n",
        "\n",
        "            for i, p in enumerate(preds_c):\n",
        "                img_id = p[\"image_id\"]\n",
        "                box_p = p[\"box\"]\n",
        "                gt_boxes_img = gt_boxes_c[img_id]\n",
        "\n",
        "                if len(gt_boxes_img) == 0:\n",
        "                    FP[i] = 1.0\n",
        "                    continue\n",
        "\n",
        "                ious = box_iou(box_p, gt_boxes_img)\n",
        "                best_idx = int(np.argmax(ious))\n",
        "                best_iou = ious[best_idx]\n",
        "\n",
        "                if best_iou >= iou_thr and not used[img_id][best_idx]:\n",
        "                    TP[i] = 1.0\n",
        "                    used[img_id][best_idx] = True\n",
        "                else:\n",
        "                    FP[i] = 1.0\n",
        "\n",
        "            cum_TP = np.cumsum(TP)\n",
        "            cum_FP = np.cumsum(FP)\n",
        "\n",
        "            rec = cum_TP / (gt_count_per_class[c] + 1e-16)\n",
        "            prec = cum_TP / (cum_TP + cum_FP + 1e-16)\n",
        "\n",
        "            ap = voc_ap(rec, prec)\n",
        "            ap_per_iou.append(ap)\n",
        "\n",
        "            if abs(iou_thr - 0.5) < 1e-6:\n",
        "                TP_50[c] = int(cum_TP[-1])\n",
        "                FP_50[c] = int(cum_FP[-1])\n",
        "                FN_50[c] = gt_count_per_class[c] - TP_50[c]\n",
        "\n",
        "        if len(ap_per_iou) > 0:\n",
        "            ap50_per_class[c] = ap_per_iou[0]\n",
        "            ap5095_per_class[c] = float(np.mean(ap_per_iou))\n",
        "\n",
        "    # ----- Aggregate metrics -----\n",
        "    precision_per_class = {}\n",
        "    recall_per_class = {}\n",
        "    f1_per_class = {}\n",
        "\n",
        "    for c in range(1, num_cls_with_bg):\n",
        "        TPc, FPc, FNc = TP_50[c], FP_50[c], FN_50[c]\n",
        "        if gt_count_per_class[c] == 0:\n",
        "            continue\n",
        "\n",
        "        prec = TPc / (TPc + FPc + 1e-16)\n",
        "        rec = TPc / (TPc + FNc + 1e-16)\n",
        "        f1 = 2 * prec * rec / (prec + rec + 1e-16)\n",
        "\n",
        "        precision_per_class[c] = prec\n",
        "        recall_per_class[c] = rec\n",
        "        f1_per_class[c] = f1\n",
        "\n",
        "    valid_classes = [c for c in range(1, num_cls_with_bg) if gt_count_per_class[c] > 0]\n",
        "    mAP50 = np.mean([ap50_per_class[c] for c in valid_classes])\n",
        "    mAP5095 = np.mean([ap5095_per_class[c] for c in valid_classes])\n",
        "    macro_f1 = np.mean([f1_per_class[c] for c in valid_classes])\n",
        "\n",
        "    print(\"\\n=== RetinaNet Validation Metrics (IoU-based) ===\")\n",
        "    print(f\"mAP@50 (macro over classes)     : {mAP50:.4f}\")\n",
        "    print(f\"mAP@50\u201395 (macro over classes)  : {mAP5095:.4f}\")\n",
        "    print(f\"Macro F1 score (IoU=0.5)        : {macro_f1:.4f}\\n\")\n",
        "\n",
        "    print(\"Per-class metrics (IoU=0.5):\")\n",
        "    for c in valid_classes:\n",
        "        name = class_names[c - 1]\n",
        "        print(f\"Class {c-1:2d} ({name:>15}) | \"\n",
        "              f\"P={precision_per_class[c]:.3f}, \"\n",
        "              f\"R={recall_per_class[c]:.3f}, \"\n",
        "              f\"F1={f1_per_class[c]:.3f}, \"\n",
        "              f\"AP50={ap50_per_class[c]:.3f}, \"\n",
        "              f\"AP50\u201395={ap5095_per_class[c]:.3f}\")\n",
        "\n",
        "    return {\n",
        "        \"mAP50\": mAP50,\n",
        "        \"mAP50_95\": mAP5095,\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"ap50_per_class\": ap50_per_class,\n",
        "        \"ap5095_per_class\": ap5095_per_class,\n",
        "        \"precision_per_class\": precision_per_class,\n",
        "        \"recall_per_class\": recall_per_class,\n",
        "        \"f1_per_class\": f1_per_class,\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------- Confusion matrix computation ----------\n",
        "def compute_confusion_matrix(model, dataset, device,\n",
        "                             iou_thr=0.5, score_thresh=0.5):\n",
        "    \"\"\"\n",
        "    Build a confusion matrix of shape (num_classes, num_classes) using\n",
        "    greedy matching between predictions and ground-truth.\n",
        "    Rows: ground-truth\n",
        "    Columns: predicted\n",
        "    Background label (0) is ignored.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    eval_loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        collate_fn=detection_collate_fn\n",
        "    )\n",
        "\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=np.int32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, (images, targets) in enumerate(eval_loader):\n",
        "            img = images[0].to(device)\n",
        "            target = targets[0]\n",
        "            gt_boxes = target[\"boxes\"].cpu().numpy()\n",
        "            gt_labels = target[\"labels\"].cpu().numpy()  # [1..num_classes]\n",
        "\n",
        "            outputs = model([img])[0]\n",
        "            pred_boxes = outputs[\"boxes\"].cpu().numpy()\n",
        "            pred_scores = outputs[\"scores\"].cpu().numpy()\n",
        "            pred_labels = outputs[\"labels\"].cpu().numpy()\n",
        "\n",
        "            # filter score & remove background\n",
        "            keep = (pred_scores >= score_thresh) & (pred_labels != 0)\n",
        "            pred_boxes = pred_boxes[keep]\n",
        "            pred_labels = pred_labels[keep]\n",
        "\n",
        "            pred_used = np.zeros(len(pred_boxes), dtype=bool)\n",
        "\n",
        "            for gt_box, gt_lab in zip(gt_boxes, gt_labels):\n",
        "                if len(pred_boxes) == 0:\n",
        "                    continue\n",
        "                ious = np.array([box_iou(gt_box, pred_boxes[k:k+1])[0]\n",
        "                                 for k in range(len(pred_boxes))])\n",
        "                best_idx = int(np.argmax(ious))\n",
        "                best_iou = ious[best_idx]\n",
        "\n",
        "                if best_iou >= iou_thr and not pred_used[best_idx]:\n",
        "                    pred_used[best_idx] = True\n",
        "                    gt_idx = int(gt_lab - 1)\n",
        "                    pred_idx = int(pred_labels[best_idx] - 1)\n",
        "                    if 0 <= gt_idx < num_classes and 0 <= pred_idx < num_classes:\n",
        "                        cm[gt_idx, pred_idx] += 1\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "# ---------- 4.1: Run evaluation ----------\n",
        "retina_metrics = evaluate_retinanet(model_retina, val_dataset, device)\n",
        "\n",
        "cm_retina = compute_confusion_matrix(model_retina, val_dataset, device,\n",
        "                                     iou_thr=0.5, score_thresh=0.5)\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.imshow(cm_retina, cmap=\"Blues\")\n",
        "plt.title(\"RetinaNet Confusion Matrix (Val, IoU=0.5, score>=0.5)\")\n",
        "plt.xlabel(\"Predicted class\")\n",
        "plt.ylabel(\"Ground-truth class\")\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ================================\n",
        "# 4.2 Visualize Results on Validation Images\n",
        "# ================================\n",
        "def visualize_retinanet_predictions(model, dataset, device,\n",
        "                                    num_samples=8, score_thresh=0.5):\n",
        "    \"\"\"\n",
        "    Display a subset of validation images with RetinaNet predictions.\n",
        "    Background label (0) is ignored.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    indices = list(range(len(dataset)))\n",
        "    random.shuffle(indices)\n",
        "    indices = indices[:num_samples]\n",
        "\n",
        "    rows = 2\n",
        "    cols = (num_samples + 1) // 2\n",
        "\n",
        "    plt.figure(figsize=(4 * cols, 4 * rows))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            img_tensor, target = dataset[idx]\n",
        "            img_input = img_tensor.to(device).unsqueeze(0)\n",
        "\n",
        "            outputs = model(img_input)[0]\n",
        "            boxes = outputs[\"boxes\"].cpu().numpy()\n",
        "            scores = outputs[\"scores\"].cpu().numpy()\n",
        "            labels = outputs[\"labels\"].cpu().numpy()\n",
        "\n",
        "            keep = (scores >= score_thresh) & (labels != 0)\n",
        "            boxes = boxes[keep]\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "\n",
        "            img_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "            img_np = (img_np * 255).astype(np.uint8).copy()\n",
        "\n",
        "            for b, s, lab in zip(boxes, scores, labels):\n",
        "                x1, y1, x2, y2 = b.astype(int)\n",
        "                cls_id = int(lab - 1)\n",
        "                color = (0, 255, 0)\n",
        "\n",
        "                cv2.rectangle(img_np, (x1, y1), (x2, y2), color, 2)\n",
        "                label_text = f\"{class_names[cls_id]} {s:.2f}\"\n",
        "                cv2.putText(img_np, label_text, (x1, max(0, y1 - 5)),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
        "\n",
        "            plt.subplot(rows, cols, i + 1)\n",
        "            plt.imshow(img_np)\n",
        "            plt.title(f\"Val idx {idx}\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "visualize_retinanet_predictions(model_retina, val_dataset, device,\n",
        "                                num_samples=8, score_thresh=0.5)\n",
        "\n",
        "# ================================\n",
        "# 4.3 Generalization Test\n",
        "# ================================\n",
        "def run_generalization_test(model, device, image_path, score_thresh=0.5):\n",
        "    \"\"\"\n",
        "    Run RetinaNet on an external traffic sign image and visualize predictions.\n",
        "    Background label (0) is ignored.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img_resized = cv2.resize(img_rgb, (RETINA_IMG_SIZE, RETINA_IMG_SIZE))\n",
        "    img_norm = img_resized.astype(np.float32) / 255.0\n",
        "    img_tensor = torch.from_numpy(img_norm).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)[0]\n",
        "\n",
        "    boxes = outputs[\"boxes\"].cpu().numpy()\n",
        "    scores = outputs[\"scores\"].cpu().numpy()\n",
        "    labels = outputs[\"labels\"].cpu().numpy()\n",
        "\n",
        "    keep = (scores >= score_thresh) & (labels != 0)\n",
        "    boxes = boxes[keep]\n",
        "    scores = scores[keep]\n",
        "    labels = labels[keep]\n",
        "\n",
        "    vis_img = img_resized.copy()\n",
        "    for b, s, lab in zip(boxes, scores, labels):\n",
        "        x1, y1, x2, y2 = b.astype(int)\n",
        "        cls_id = int(lab - 1)\n",
        "        color = (0, 255, 0)\n",
        "        cv2.rectangle(vis_img, (x1, y1), (x2, y2), color, 2)\n",
        "        label_text = f\"{class_names[cls_id]} {s:.2f}\"\n",
        "        cv2.putText(vis_img, label_text, (x1, max(0, y1 - 5)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(vis_img)\n",
        "    plt.title(f\"Generalization Test: {image_path}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Generate Submissions with IMPROVED Model (v2)\n",
        "# ================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"GENERATING SUBMISSIONS WITH IMPROVED MODEL (v2)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "sample_csv_path = Path(root, \"sample_submission.csv\")\n",
        "\n",
        "# Generate submissions with improved model\n",
        "configs_v2 = [\n",
        "    {\"score_thresh\": 0.05, \"nms_thresh\": 0.45, \"suffix\": \"v2_very_low\"},\n",
        "    {\"score_thresh\": 0.10, \"nms_thresh\": 0.50, \"suffix\": \"v2_low\"},\n",
        "    {\"score_thresh\": 0.15, \"nms_thresh\": 0.50, \"suffix\": \"v2_med\"},\n",
        "    {\"score_thresh\": 0.20, \"nms_thresh\": 0.50, \"suffix\": \"v2_med_high\"},\n",
        "]\n",
        "\n",
        "for config in configs_v2:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Improved Model Config: score={config['score_thresh']}, nms={config['nms_thresh']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    out_csv_path = Path(OUTPUT_PATH, f\"submission_retinanet_{config['suffix']}.csv\")\n",
        "\n",
        "    generate_submission_retinanet_improved(\n",
        "        model=model_retina_v2,  # Use improved model\n",
        "        sample_csv_path=sample_csv_path,\n",
        "        out_csv_path=out_csv_path,\n",
        "        device=device,\n",
        "        score_thresh=config['score_thresh'],\n",
        "        nms_thresh=config['nms_thresh'],\n",
        "        img_size=RETINA_IMG_SIZE,\n",
        "        num_classes=num_classes,\n",
        "    )\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ALL SUBMISSIONS GENERATED!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nGenerated submission files:\")\n",
        "print(\"  1. submission_retinanet_v2_very_low.csv  (score_thresh=0.05)\")\n",
        "print(\"  2. submission_retinanet_v2_low.csv       (score_thresh=0.10)\")\n",
        "print(\"  3. submission_retinanet_v2_med.csv       (score_thresh=0.15) <- RECOMMENDED\")\n",
        "print(\"  4. submission_retinanet_v2_med_high.csv  (score_thresh=0.20)\")\n",
        "print(\"\\nRecommendation: Start with 'v2_med' for best balance.\")\n",
        "print(\"If score is too low, try 'v2_low'. If too many false positives, try 'v2_med_high'.\")"
      ],
      "metadata": {
        "id": "RfYcvMivVUUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw7tQldhRuRk"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Part 5: Generate Kaggle Submission using RetinaNet Predictions (IMPROVED)\n",
        "# ================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "from torchvision.ops import nms\n",
        "\n",
        "\n",
        "def generate_submission_retinanet_improved(\n",
        "    model,\n",
        "    sample_csv_path,\n",
        "    out_csv_path,\n",
        "    device,\n",
        "    score_thresh=0.05,\n",
        "    nms_thresh=0.5,\n",
        "    img_size=416,\n",
        "    num_classes=14,\n",
        "):\n",
        "    \"\"\"\n",
        "    IMPROVED: Generate a Kaggle submission CSV with proper NMS and preprocessing.\n",
        "\n",
        "    Key improvements:\n",
        "    1. Explicit resize to match training size (416x416)\n",
        "    2. NMS to remove duplicate detections\n",
        "    3. Proper coordinate normalization\n",
        "    4. Better handling of missing predictions\n",
        "\n",
        "    Parameters:\n",
        "        model:           Trained RetinaNet model\n",
        "        sample_csv_path: Path to sample_submission.csv\n",
        "        out_csv_path:    Path to save generated submission CSV\n",
        "        device:          torch.device (\"cuda\" or \"cpu\")\n",
        "        score_thresh:    Minimum confidence score (try 0.05-0.3)\n",
        "        nms_thresh:      NMS IoU threshold (typically 0.4-0.6)\n",
        "        img_size:        Input size model was trained on (416)\n",
        "        num_classes:     Number of classes (14 for traffic signs)\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Load sample submission\n",
        "    df_sub = pd.read_csv(sample_csv_path)\n",
        "    print(\"Loaded sample_submission with rows:\", len(df_sub))\n",
        "\n",
        "    # Extract unique image IDs\n",
        "    unique_image_ids = sorted(set(df_sub[\"ID\"].apply(lambda s: s.rsplit(\"_\", 1)[0])))\n",
        "    print(\"Unique test images found:\", len(unique_image_ids))\n",
        "\n",
        "    # Store predictions per image\n",
        "    pred_dict = {}\n",
        "\n",
        "    # Run inference on all test images\n",
        "    with torch.no_grad():\n",
        "        for img_id in unique_image_ids:\n",
        "            img_path = Path(TEST_IMAGES, f\"{img_id}.jpg\")\n",
        "\n",
        "            if not img_path.exists():\n",
        "                print(f\"[Warning] Test image not found: {img_path}\")\n",
        "                pred_dict[img_id] = []\n",
        "                continue\n",
        "\n",
        "            # Load image\n",
        "            img = cv2.imread(str(img_path))\n",
        "            if img is None:\n",
        "                pred_dict[img_id] = []\n",
        "                continue\n",
        "\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            orig_h, orig_w = img_rgb.shape[:2]\n",
        "\n",
        "            # CRITICAL FIX: Resize to training size (416x416)\n",
        "            img_resized = cv2.resize(img_rgb, (img_size, img_size), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "            # Convert to tensor and normalize to [0, 1]\n",
        "            img_tensor = torch.from_numpy(img_resized).float() / 255.0\n",
        "            img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(img_tensor)[0]\n",
        "            boxes = outputs[\"boxes\"].detach().cpu()\n",
        "            scores = outputs[\"scores\"].detach().cpu()\n",
        "            labels = outputs[\"labels\"].detach().cpu()\n",
        "\n",
        "            # Filter by score and remove background (label == 0)\n",
        "            keep_score = (scores >= score_thresh) & (labels != 0)\n",
        "            boxes = boxes[keep_score]\n",
        "            scores = scores[keep_score]\n",
        "            labels = labels[keep_score]\n",
        "\n",
        "            if len(boxes) == 0:\n",
        "                pred_dict[img_id] = []\n",
        "                continue\n",
        "\n",
        "            # CRITICAL FIX: Apply NMS per class to remove duplicate detections\n",
        "            final_boxes = []\n",
        "            final_scores = []\n",
        "            final_labels = []\n",
        "\n",
        "            unique_labels = torch.unique(labels)\n",
        "            for label in unique_labels:\n",
        "                label_mask = labels == label\n",
        "                label_boxes = boxes[label_mask]\n",
        "                label_scores = scores[label_mask]\n",
        "\n",
        "                # Apply NMS\n",
        "                keep_nms = nms(label_boxes, label_scores, nms_thresh)\n",
        "\n",
        "                final_boxes.append(label_boxes[keep_nms])\n",
        "                final_scores.append(label_scores[keep_nms])\n",
        "                final_labels.append(torch.full((len(keep_nms),), label, dtype=torch.int64))\n",
        "\n",
        "            if len(final_boxes) > 0:\n",
        "                boxes = torch.cat(final_boxes)\n",
        "                scores = torch.cat(final_scores)\n",
        "                labels = torch.cat(final_labels)\n",
        "            else:\n",
        "                pred_dict[img_id] = []\n",
        "                continue\n",
        "\n",
        "            # Sort by score (highest first)\n",
        "            sorted_indices = torch.argsort(scores, descending=True)\n",
        "            boxes = boxes[sorted_indices].numpy()\n",
        "            scores = scores[sorted_indices].numpy()\n",
        "            labels = labels[sorted_indices].numpy()\n",
        "\n",
        "            # Convert to YOLO format (normalized coordinates)\n",
        "            preds = []\n",
        "            for b, scr, lab in zip(boxes, scores, labels):\n",
        "                x1, y1, x2, y2 = b\n",
        "\n",
        "                # CRITICAL FIX: Boxes are in img_size space (416x416),\n",
        "                # normalize by img_size to get [0, 1] range\n",
        "                bw = max(x2 - x1, 1e-6)\n",
        "                bh = max(y2 - y1, 1e-6)\n",
        "                xc = x1 + bw / 2.0\n",
        "                yc = y1 + bh / 2.0\n",
        "\n",
        "                # Normalize by image size (416)\n",
        "                xc_norm = float(xc / img_size)\n",
        "                yc_norm = float(yc / img_size)\n",
        "                bw_norm = float(bw / img_size)\n",
        "                bh_norm = float(bh / img_size)\n",
        "\n",
        "                # Clip to [0, 1] range\n",
        "                xc_norm = np.clip(xc_norm, 0, 1)\n",
        "                yc_norm = np.clip(yc_norm, 0, 1)\n",
        "                bw_norm = np.clip(bw_norm, 0, 1)\n",
        "                bh_norm = np.clip(bh_norm, 0, 1)\n",
        "\n",
        "                # Convert label from [1, num_classes] to [0, num_classes-1]\n",
        "                class_label = int(lab) - 1\n",
        "                class_label = max(0, min(num_classes - 1, class_label))\n",
        "\n",
        "                preds.append({\n",
        "                    \"class_label\": class_label,\n",
        "                    \"x_center\": xc_norm,\n",
        "                    \"y_center\": yc_norm,\n",
        "                    \"width\": bw_norm,\n",
        "                    \"height\": bh_norm,\n",
        "                    \"score\": float(scr)\n",
        "                })\n",
        "\n",
        "            pred_dict[img_id] = preds\n",
        "\n",
        "    # Build output rows matching sample_submission.csv\n",
        "    out_rows = []\n",
        "\n",
        "    for _id in df_sub[\"ID\"].values:\n",
        "        img_id, idx_str = _id.rsplit(\"_\", 1)\n",
        "        k = int(idx_str)\n",
        "\n",
        "        preds = pred_dict.get(img_id, [])\n",
        "\n",
        "        if k < len(preds):\n",
        "            p = preds[k]\n",
        "            out_rows.append([\n",
        "                _id,\n",
        "                p[\"class_label\"],\n",
        "                p[\"x_center\"],\n",
        "                p[\"y_center\"],\n",
        "                p[\"width\"],\n",
        "                p[\"height\"],\n",
        "            ])\n",
        "        else:\n",
        "            # Placeholder for missing predictions\n",
        "            out_rows.append([\n",
        "                _id,\n",
        "                0,\n",
        "                0.5,\n",
        "                0.5,\n",
        "                0.01,  # Very small box instead of 0\n",
        "                0.01,\n",
        "            ])\n",
        "\n",
        "    # Create DataFrame and save\n",
        "    df_out = pd.DataFrame(\n",
        "        out_rows,\n",
        "        columns=[\"ID\", \"class_label\", \"x_center\", \"y_center\", \"width\", \"height\"]\n",
        "    )\n",
        "\n",
        "    print(\"Final submission rows:\", len(df_out))\n",
        "    df_out.to_csv(out_csv_path, index=False)\n",
        "    print(\"Saved improved submission to:\", out_csv_path)\n",
        "\n",
        "    # Print statistics\n",
        "    total_preds = sum(len(preds) for preds in pred_dict.values())\n",
        "    images_with_preds = sum(1 for preds in pred_dict.values() if len(preds) > 0)\n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"  Total predictions: {total_preds}\")\n",
        "    print(f\"  Images with predictions: {images_with_preds}/{len(unique_image_ids)}\")\n",
        "    print(f\"  Avg predictions per image: {total_preds/len(unique_image_ids):.2f}\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Generate submission with different threshold configurations\n",
        "# ================================\n",
        "print(\"=\" * 70)\n",
        "print(\"GENERATING IMPROVED SUBMISSIONS WITH OPTIMIZED PARAMETERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "sample_csv_path = Path(root, \"sample_submission.csv\")\n",
        "\n",
        "# Try multiple configurations to find the best one\n",
        "configs = [\n",
        "    {\"score_thresh\": 0.05, \"nms_thresh\": 0.5, \"suffix\": \"low_thresh\"},\n",
        "    {\"score_thresh\": 0.15, \"nms_thresh\": 0.5, \"suffix\": \"med_thresh\"},\n",
        "    {\"score_thresh\": 0.25, \"nms_thresh\": 0.5, \"suffix\": \"high_thresh\"},\n",
        "]\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Config: score_thresh={config['score_thresh']}, nms_thresh={config['nms_thresh']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    out_csv_path = Path(OUTPUT_PATH, f\"submission_retinanet_improved_{config['suffix']}.csv\")\n",
        "\n",
        "    generate_submission_retinanet_improved(\n",
        "        model=model_retina,\n",
        "        sample_csv_path=sample_csv_path,\n",
        "        out_csv_path=out_csv_path,\n",
        "        device=device,\n",
        "        score_thresh=config['score_thresh'],\n",
        "        nms_thresh=config['nms_thresh'],\n",
        "        img_size=RETINA_IMG_SIZE,\n",
        "        num_classes=num_classes,\n",
        "    )\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"DONE! Generated 3 submission files with different thresholds.\")\n",
        "print(\"Try submitting each to Kaggle to see which performs best!\")\n",
        "print(f\"{'='*70}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}